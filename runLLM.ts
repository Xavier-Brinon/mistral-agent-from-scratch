import assert from 'node:assert/strict'
import { Mistral } from '@mistralai/mistralai'
import { mistralModels } from './models.ts'
import type { ChatCompletionResponse } from '@mistralai/mistralai/models/components/chatcompletionresponse'
import type { RequestOptions } from '@mistralai/mistralai/lib/sdks'
import { logger } from './logger.ts'

const runLLMLogger = logger.child({ module: 'runLLM' })

const apiKey = process.env.MISTRAL_API_KEY
assert.ok(apiKey, 'MISTRAL_API_KEY seem to be missing')
assert.ok(apiKey !== '', 'MISTRAL_API_KEY is empty')

const client = new Mistral({ apiKey })

/**
 * Sends the given prompt to a Mistal model.
 * Returns the response generated by the model.
 * Example: runLLM('roses are red')
 *
 *   chatResponse: '{\n' +
    '  "id": "a1beee8af5cb4c12bdc09fa08b8a3b4e",\n' +
    '  "object": "chat.completion",\n' +
    '  "model": "open-mistral-nemo",\n' +
    '  "usage": {\n' +
    '    "promptTokens": 8,\n' +
    '    "completionTokens": 18,\n' +
    '    "totalTokens": 26\n' +
    '  },\n' +
    '  "created": 1737128975,\n' +
    '  "choices": [\n' +
    '    {\n' +
    '      "index": 0,\n' +
    '      "message": {\n' +
    '        "content": "Violets are blue, Sugar is sweet, And so are you! ðŸ˜Š",\n' +
    '        "toolCalls": null,\n' +
    '        "prefix": false,\n' +
    '        "role": "assistant"\n' +
    '      },\n' +
    '      "finishReason": "stop"\n' +
    '    }\n' +
    '  ]\n' +
    '}'
 */
export const runLLM = async (prompt: string, options: RequestOptions): Promise<ChatCompletionResponse> => {
  assert.ok(typeof prompt === 'string', `prompt should be a string, not ${typeof prompt}`)
  assert.ok(prompt !== '', 'prompt should not be an empty string')

  const chatResponse = await client.chat.complete({
    model: mistralModels.MistralNemo,
    messages: [
      {
	role: 'user',
	content: prompt
      }
    ]
  }, options)

  assert.ok(chatResponse, 'mistral failed to respond')
  runLLMLogger.debug({ chatResponse })
  return chatResponse
}
