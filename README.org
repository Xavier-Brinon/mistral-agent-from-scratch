#+title: Build an AI Agent from Scratch with Mistral
#+date: [2025-01-16 Thu]
#+startup: indent
#+property: header-args :results output
* mistral-agent-from-scratch
Like the Front End Masters Course, build an AI agent from scratch, but with
Mistral.
* Start a node project with TS
Since the last version of *Node*, as of today, support TS, I'll just let the IDE
handle the types and run without transpiling.
In the root of the folder:
#+begin_src bash
  npm init -y
#+end_src

Some basic settings to begin with:
- Main entry point is =index.ts=. I write directly in typescript and let *Node*
  do the stripping. I will probably have to use only a subset of the TS features
  available.
- Tests will run with the built in test runner ~node --test~.
  *Node* will run against any file that ends with =.test.ts=


Because Typescript doesn't understand *Node* types, I need to also install them.
#+name: install node types
#+begin_src bash
  npm i -D @types/node
#+end_src
* Env
To work with *Mistral*, I need an API key. I will also need to use it as an Env
variable. *DotEnvX* kicks ass so I'll go with this one.
See https://dotenvx.com/docs/quickstart
** Create the .env files
- .env :: as a template listing all the variables but leave them empty
- .env.local :: using the keys for the test environment
- .env.production :: using the keys for production
** Encrypt all values
let's do a quick test:
#+name: set env var
#+begin_src bash
  dotenvx set HELLO "local" -f .env.local
#+end_src

#+RESULTS: set env var
: ✔ set HELLO with encryption (.env.local)
: ✔ key added to .env.keys (DOTENV_PRIVATE_KEY_LOCAL)
: ℹ add .env.keys to .gitignore: [echo ".env.keys" >> .gitignore]
: ℹ run [DOTENV_PRIVATE_KEY_LOCAL='...' dotenvx get HELLO] to test decryption locally

It stores the public key and the encrypted value in the =.env.local=, it can be
gitignored as well as the local version is by definition local only.
The production one can be pushed to the repo as long as the private key remains
private.
⚜️ DotEnvX finds the .env.keys file and use it to decrypt the file, so you can
run ~dotenvx get HELLO -f .env.local~ to display the decrypted value.
** Mistral API Keys
I create 2 keys, one for local and one for production.
Set them up in =.env.local= and =.env.production=.
* Mistral API
Based on the Mistral's [[https://docs.mistral.ai/getting-started/quickstart/#getting-started-with-mistral-ai-api][quickstart]],
I create a script to call and display the chat response.
** NPM package
Install the *Mistral* npm package
#+name: install mistral packagge
#+begin_src bash
  npm i @mistralai/mistralai
#+end_src
** Models
The example given is using the latest modal, but there are free, older ones.
- Pixtral :: pixtral-12b-2409
- Mistral Nemo :: open-mistral-nemo
- Codestral Mamba :: open-codestral-mamba


I create a const file with those models available.
* Reading inputs
There is an [[https://nodejs.org/docs/latest/api/readline.html#readline][example]] on *Node* documentation.
* TigerBeetle
I'm trying to follow some common sense based on the TigerBeetle [[https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md][manifesto]].
mainly the part on *Assertions*
#+begin_quote
Assert all function arguments and return values, pre/postconditions and
invariants. A function must not operate blindly on data it has not checked.
The purpose of a function is to increase the probability that a program is
correct.
Assertions within a function are part of how functions serve this purpose.
The assertion density of the code must average a minimum of two assertions per
function.
#+end_quote
* TODO Prompt can be passed via the cli                                 :CLI:
Right now the command is ~npm run start~. It waits for the user input via
~readline.createInterface~. Instead I want to be able to add the prompt
when calling the LLM, something like ~npm run llm "this is the prompt"~
Checked [[https://nodejs.org/docs/latest/api/process.html#processargv][manual]], and I don't see yet how this can work when the command
triggers an npm command that calls the function:
1. npm run start
2. node index.ts


The ~process.argv~ returns the argv of (1.)
For now I'll get the prompt via the ~readline~ interface.
* TODO Read the article about the alternatives to enum              :ARTICLE:
Axel wrote an [[https://2ality.com/2025/01/typescript-enum-patterns.html][article]] about the Typescript ~enum~ and its alternatives.
Could be relevant when listing the Mistral models.
* DONE Handle ~SIGTERM~
* Ollama
I think it would be actually funnier to do it 100% local using Ollama
Let's make a new repo!
